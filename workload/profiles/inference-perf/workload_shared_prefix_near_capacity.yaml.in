# EXPERIMENT D — Near-capacity (50k tokens/pod)
# Same math as fits-in-cache, safe fit ≈ 56 groups cluster-wide.
# ► num_groups: 65 (~16/pod vs 14 slots/pod) for mild KV pressure.
#   Still safe at high QPS with short prompts.
load:
  type: constant
  stages:
    - rate: 3
      duration: 60
    - rate: 8
      duration: 75
    - rate: 12
      duration: 75
    - rate: 16
      duration: 90
    - rate: 21
      duration: 105
    - rate: 24
      duration: 120

api:
  type: completion
  streaming: true

server:
  type: vllm
  model_name: REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL
  base_url: REPLACE_ENV_LLMDBENCH_HARNESS_STACK_ENDPOINT_URL
  ignore_eos: true

tokenizer:
  pretrained_model_name_or_path: REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_TOKENIZER

data:
  type: shared_prefix
  shared_prefix:
    num_groups: 65
    num_prompts_per_group: 3
    system_prompt_len: 3000
    question_len: 500
    output_len: 256

report:
  request_lifecycle:
    summary: true
    per_stage: true
    per_request: true

storage:
  local_storage:
    path: /workspace