endpoint:
  stack_name: my-stack            # user defined name for the stack (results prefix)
  model: &model Qwen/Qwen3.2B     # Exact HuggingFace model name. Must match stack deployed.
  &namespace namespace: dean-ns
  base_url: &url http://qwen-modelservice.dean-ns.svc.cluster.local:8080  # Base URL of inference endpoint
  hf_token: &hf_token hf_WsXGfXJtYgkzQvYtYxWJvR     # TODO replace with environment variable reference or secret name
  model_pvc_name: model-pvc       # PVC where model files are cached


control:
  root_dir: $(realpath $(pwd)/..) # root of git clone -- DO WE NEED THIS????
  work_dir:                       # working directory to store temporary and autogenerated files. 
                                  # Do not edit content manually.
                                  # If not set, a temp directory will be created.


harness:
  name: inference-perf
  results_pvc_name: workload-pvc  # PVC where benchmark results are stored
  namespace: *namespace           # Namespace where harness is deployed. Typically with stack.
  parallelism: 1                  # Number of parallel workload launcher pods to create.  
  wait_timeout: 600               # Time (in seconds) to wait for workload launcher pod to complete before terminating.
                                  # Set to 0 to disable timeout.
                                  # Note: workload launcher pod will continue running in cluster if timeout occurs.
  image: ghcr.io/llm-d/llm-d-benchmark:latest # llm-d-benchmark harness image 
                                  # @TODO do not use latest in production, use specific version tag.

workload:                         # yaml configuration for harness workload(s)
  
  # an example workload using random synthetic data
  sanity_random:
    load:
      type: constant
      stages:
      - rate: 1
        duration: 30
    api:
      type: completion
      streaming: true
    server:
      type: vllm
      model_name: *model
      base_url: *url
      ignore_eos: true
    tokenizer:
      pretrained_model_name_or_path: *model
    data:
      type: random
      input_distribution:
        min: 10             # min length of the synthetic prompts
        max: 100            # max length of the synthetic prompts
        mean: 50            # mean length of the synthetic prompts
        std: 10             # standard deviation of the length of the synthetic prompts
        total_count: 100    # total number of prompts to generate to fit the above mentioned distribution constraints
      output_distribution:
        min: 10             # min length of the output to be generated
        max: 100            # max length of the output to be generated
        mean: 50            # mean length of the output to be generated
        std: 10             # standard deviation of the length of the output to be generated
        total_count: 100    # total number of output lengths to generate to fit the above mentioned distribution constraints
      # dataset_url: https://huggingface.co/datasets/deanli/llm-d-benchmark-datasets/resolve/main/small_instruction_following_dataset.jsonl
    report:
      request_lifecycle:
        summary: true
        per_stage: true
        per_request: true
    storage:
      local_storage:
        path: /workspace

  # an example workload using shared prefix synthetic data
  shared_prefix_synthetic:
    load:
      type: constant
      stages:
      - rate: 2
        duration: 50
      - rate: 5
        duration: 50
      - rate: 8
        duration: 50
      - rate: 10
        duration: 50
      - rate: 12
        duration: 50
      - rate: 15
        duration: 50
      - rate: 20
        duration: 50
    api:
      type: completion
      streaming: true
    server:
      type: vllm
      model_name: REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL
      base_url: REPLACE_ENV_LLMDBENCH_HARNESS_STACK_ENDPOINT_URL
      ignore_eos: true
    tokenizer:
      pretrained_model_name_or_path: REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_TOKENIZER
    data:
      type: shared_prefix
      shared_prefix:
        num_groups: 32                # Number of distinct shared prefixes
        num_prompts_per_group: 32     # Number of unique questions per shared prefix
        system_prompt_len: 2048       # Length of the shared prefix (in tokens)
        question_len: 256             # Length of the unique question part (in tokens)
        output_len: 256               # Target length for the model's generated output (in tokens)
    report:
      request_lifecycle:
        summary: true
        per_stage: true
        per_request: true
    storage:
      local_storage:
        path: /workspace